{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_crawl_v2",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJsgIRtTpmmWIo6sKrQ4u6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimmjen/Colab/blob/master/final_crawl_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhPOum4lMb-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"주식종목 뉴스\"\"\"\n",
        "\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "# 셀레니움\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "os.chdir('./')\n",
        "\n",
        "def crawler(company_code , maxpage):\n",
        "\n",
        "    # url = 'https://finance.naver.com/item/news_news.nhn?code='+str(company_code) + '&page=' + str(page)\n",
        "    # new_source = bs(url, 'lxml')\n",
        "    # navi = new_source.find(\"table\", class_=\"Nnavi\")\n",
        "    # navi_last = navi.find(\"td\", class_=\"pgRR\")\n",
        "    # pag = navi_last.a.get('href').rsplit('&')[1]\n",
        "    # pg_last = page.split('=')[1]\n",
        "    # pg_last = int(pg_last)\n",
        "    # print(\"총\" + str(pg_last) + \"개 확인\")\n",
        "    # maxpage = pg_last\n",
        "    page = 1\n",
        "    list_news = []\n",
        "\n",
        "    while page < pg_last:\n",
        "        # 뉴스제목\n",
        "        titles = html.select('.title')\n",
        "        title_result=[]\n",
        "        for title in titles:\n",
        "            title = title.get_text()\n",
        "            title = re.sub('\\n','',title)\n",
        "            title_result.append(title)\n",
        "\n",
        "        # 뉴스 링크\n",
        "        links = html.select('.title')\n",
        "\n",
        "        link_result = []\n",
        "        for link in links:\n",
        "            add = 'https://finance.naver.com'+link.find('a')['href']\n",
        "            link_result.append(add)\n",
        "\n",
        "        # 뉴스 날짜\n",
        "        dates = html.select('.date')\n",
        "        date_result = [date.get_text() for date in dates]\n",
        "\n",
        "        # 뉴스 매체\n",
        "        sources = html.select('.info')\n",
        "        source_result = [source.get_text() for source in sources]\n",
        "\n",
        "        # 변수들을 합쳐서 해당 디렉토리에 csv 파일로 저장하기\n",
        "        results = {'날짜' : date_result, '언론사': source_result, '기사제목': title_result, '링크': link_result}\n",
        "        list_news.append(results)\n",
        "        # df_result = pd.DataFrame(result)\n",
        "\n",
        "        print('다운중~')\n",
        "        \n",
        "        page += 1\n",
        "    df_result = pd.DataFrame(list_news)\n",
        "    df_result.to_csv('./' + 'news_'+ company_code +'.csv', encoding='utf-8')\n",
        "\n",
        "# def convert_to_code(company, maxpage):\n",
        "\n",
        "#     data = pd.read_csv('company_list.csv', dtype=str, sep='\\t')\n",
        "#     company_name = data['회사명']\n",
        "#     keys = [i for i in company_name]\n",
        "\n",
        "#     company_code = data['종목코드']\n",
        "#     values = [j for j in company_code]\n",
        "\n",
        "#     dict_result = dict(zip(keys, values))\n",
        "\n",
        "#     pattern = '[0-9a-zA-Z가-힣]+'\n",
        "\n",
        "#     if bool(re.match(pattern, company)) == True:\n",
        "#         company_code = dict_result.get(str(company))\n",
        "#         crawler(company_code, maxpage)\n",
        "\n",
        "#     else:\n",
        "#         company_code = str(company)\n",
        "#         crawler(company_code, maxpage)\n",
        "\n",
        "def main():\n",
        "    info_main = input(\"=\"*50 + \"\\n\" + \"실시간 뉴스기사 다운받기\" + \"\\n\" + \" 시작하시려면 Enter를 눌러주세요.\" + \"\\n\" + \"=\"*50)\n",
        "\n",
        "    company_code = input(\"종목이나 이름이나 코드 입력 : \")\n",
        "\n",
        "    url = 'https://finance.naver.com/item/news_news.nhn?code='+str(company_code)\n",
        "    driver = webdriver.Chrome('./chromedriver')\n",
        "    driver.get(url)\n",
        "    mydata = driver.find_element_by_class_name('type5')\n",
        "    mydata = driver.page_source\n",
        "\n",
        "    html = bs(mydata, 'lxml')\n",
        "    # 크롤링할 페이지 마지막페이지 확인\n",
        "    navi = html.find(\"table\", class_=\"Nnavi\")\n",
        "    navi_last = navi.find(\"td\", class_=\"pgRR\")\n",
        "    pag = navi_last.a.get('href').rsplit('&')[1]\n",
        "    pg_last = pag.split('=')[1]\n",
        "    pg_last = int(pg_last)\n",
        "    print(\"총 \" + str(pg_last) + \" 개 확인\")\n",
        "    \n",
        "    maxpage = input(\"총 페이지 수 : {page}\".format(page=pg_last))\n",
        "\n",
        "    # convert_to_code(company, maxpage)\n",
        "main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}